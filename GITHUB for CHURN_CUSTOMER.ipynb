{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyhdb\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import NVARCHAR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "churn = pd.read_csv('data_20171001_20190930.csv', chunksize=100000, iterator=True)\n",
    "data = pd.concat(churn, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data.date, format=\"%Y%m%d\")\n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "# lifetimes provides a transaction log -> rfm util function\n",
    "training = summary_data_from_transaction_data(\n",
    "     data,\n",
    "    'customer_id',\n",
    "    'date',\n",
    "    monetary_value_col = 'amount',\n",
    "    observation_period_end=pd.to_datetime('2018-09-30'),\n",
    "    freq='D'\n",
    ")\n",
    "training.to_csv('training_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Training RFM(20171001~20180930) anad Test RFM(20181001~20190930)\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predicting the customer future transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Delete the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "training['frequency'].plot(kind='hist', bins=50)\n",
    "plt.ylabel('Number of Customers (N=1.52million)')\n",
    "plt.xlabel('Numbers of repeated daily shopping')\n",
    "plt.title('Numbers of repeated daily shopping by NWNZ clubcard customers from 2017-10-01 to 2018-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([0,1000])\n",
    "\n",
    "ax = training['monetary_value'].plot(kind='hist', bins=1000)\n",
    "\n",
    "formatter = ticker.FormatStrFormatter('$%0.1f') #declaring the formatter with the $ sign and y_values with 1 decimalplace\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.ylabel('Number of Customers (N=1.52million)')\n",
    "plt.xlabel('Average daily purchased spent')\n",
    "plt.title('Average daily purchased spent by NWNZ clubcard customers from 2017-10-01 to 2018-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the frequency between 150 <\n",
    "import matplotlib.pyplot as plt\n",
    "training[(training['frequency'] >= 141) &(training['frequency'] <= 361)]['frequency'].plot(kind='hist', bins=50)\n",
    "plt.ylabel('Number of Customers (N=1.52million)')\n",
    "plt.xlabel('Numbers of repeated daily shopping')\n",
    "plt.title('Numbers of repeated daily shopping by NWNZ clubcard customers from 2017-10-01 to 2018-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the monetary value $155<\n",
    "import matplotlib.pyplot as plt\n",
    "ax = training[(training['monetary_value'] >= 155) &(training['monetary_value'] <= 1000)]['monetary_value'].plot(kind='hist', bins=50)\n",
    "formatter = ticker.FormatStrFormatter('$%0.1f') #declaring the formatter with the $ sign and y_values with 1 decimalplace\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.ylabel('Number of Customers (N=1.52million)')\n",
    "plt.xlabel('Average daily purchased spent')\n",
    "plt.title('Average daily purchased spent by NWNZ clubcard customers from 2017-10-01 to 2018-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_describe = training.describe()[['frequency','monetary_value']]\n",
    "training_describe.columns = ['Numbers of repeated daily shopping','Average daily purchased spent']\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "training_describe['Average daily purchased spent'] = training_describe['Average daily purchased spent'].map('${:,.2f}'.format)\n",
    "training_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Outliers\n",
    "training = training.loc[(training['frequency'] > 0) & (training['frequency'] <= 250)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the BG/NBD model\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "bgf.fit(training['frequency'], training['recency'], training['T'])\n",
    "bgf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Customer Nuber of Repeated Daily Purchased prediction for next 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for next 1 years = 365 days\n",
    "training = training[training['frequency']>0]\n",
    "t = 365\n",
    "training['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, training['frequency'], training['recency'], training['T'])\n",
    "training.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Calibration dataset VS holdout dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Training RFM(20171001~20180930) anad Test RFM(20181001~20190930)\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "# Creating the Calibration_Holdout dataset\n",
    "training.columns = ['frequency_cal','recency_cal','T_cal','monetary_value_cal']\n",
    "test.columns = ['Actual Repeated Daily Purchases','recency_holdout','duration_holdout','monetary_value_holdout']\n",
    "summary_cal_holdout = pd.concat([training[['frequency_cal','recency_cal','T_cal']],test[['Actual Repeated Daily Purchases','duration_holdout']]], axis =1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_holdout.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the outliers\n",
    "summary_cal_holdout = summary_cal_holdout.query('frequency_cal <= 250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the calibration_purchases_vs_holdout_purchases Graph\n",
    "def plot_calibration_purchases_vs_holdout_purchases(\n",
    "    model, calibration_holdout_matrix, kind=\"frequency_cal\", n=7, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot calibration purchases vs holdout.\n",
    "    This currently relies too much on the lifetimes.util calibration_and_holdout_data function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: lifetimes model\n",
    "        A fitted lifetimes model.\n",
    "    calibration_holdout_matrix: pandas DataFrame\n",
    "        DataFrame from calibration_and_holdout_data function.\n",
    "    kind: str, optional\n",
    "        x-axis :\"frequency_cal\". Purchases in calibration period,\n",
    "                 \"recency_cal\". Age of customer at last purchase,\n",
    "                 \"T_cal\". Age of customer at the end of calibration period,\n",
    "                 \"time_since_last_purchase\". Time since user made last purchase\n",
    "    n: int, optional\n",
    "        Number of ticks on the x axis\n",
    "    Returns\n",
    "    -------\n",
    "    axes: matplotlib.AxesSubplot\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    x_labels = {\n",
    "        \"frequency_cal\": \"Repeated Daily Purchases from 20171001 to 20180930\",\n",
    "        \"recency_cal\": \"Age of customer at last purchase\",\n",
    "        \"T_cal\": \"Age of customer at the end of calibration period\",\n",
    "        \"time_since_last_purchase\": \"Time since user made last purchase\",\n",
    "    }\n",
    "    summary = calibration_holdout_matrix.copy()\n",
    "    duration_holdout = summary.iloc[0][\"duration_holdout\"]\n",
    "\n",
    "    summary[\"Prediction Repeated Daily Purchases\"] = model.conditional_expected_number_of_purchases_up_to_time(\n",
    "            duration_holdout, summary[\"frequency_cal\"], summary[\"recency_cal\"], summary[\"T_cal\"])\n",
    "\n",
    "    if kind == \"time_since_last_purchase\":\n",
    "        summary[\"time_since_last_purchase\"] = summary[\"T_cal\"] - summary[\"recency_cal\"]\n",
    "        ax = (\n",
    "            summary.groupby([\"time_since_last_purchase\"])[[\"Actual Repeated Daily Purchases\", \"Prediction Repeated Daily Purchases\"]]\n",
    "            .mean()\n",
    "            .iloc[:n]\n",
    "            .plot(**kwargs)\n",
    "        )\n",
    "    else:\n",
    "        ax = summary.groupby(kind)[[\"Actual Repeated Daily Purchases\", \"Prediction Repeated Daily Purchases\"]].mean().iloc[:n].plot(**kwargs)\n",
    "\n",
    "    plt.title(\"Actual Repeated Daily Purchases vs Prediction Repeated Daily Purchases\")\n",
    "    plt.xlabel(x_labels[kind])\n",
    "    plt.ylabel(\"Average of Repeated Daily Purchases from 20181001 to 20190930\")\n",
    "    plt.legend()\n",
    "\n",
    "    return ax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "bgf.fit(summary_cal_holdout['frequency_cal'], summary_cal_holdout['recency_cal'], summary_cal_holdout['T_cal'])\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "plot_calibration_purchases_vs_holdout_purchases(bgf, summary_cal_holdout, n=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_holdout = summary_cal_holdout[summary_cal_holdout['frequency_cal']>0]\n",
    "t = 365\n",
    "summary_cal_holdout['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, summary_cal_holdout['frequency_cal'], summary_cal_holdout['recency_cal'], summary_cal_holdout['T_cal'])\n",
    "holdout = summary_cal_holdout['Actual Repeated Daily Purchases'].sum(axis = 0)\n",
    "prediction = summary_cal_holdout['predicted_purchases'].sum(axis = 0)\n",
    "(prediction-holdout)/holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predicting the customer probability of Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculating the probability of Churn at the 20190930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Training and test dataset\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "#Delete outliers\n",
    "training = training.loc[(training['frequency'] > 0) & (training['frequency'] <= 250)]\n",
    "\n",
    "# Using the BG/NBD model\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "bgf.fit(training['frequency'], training['recency'], training['T'])\n",
    "bgf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict that customers still alive on the date of 2019-09-30\n",
    "test_probability_alive = test[test['frequency']>0]\n",
    "test_probability_alive['Churn'] = 1 - bgf.conditional_probability_alive(test_probability_alive['frequency'], test_probability_alive['recency'], test_probability_alive['T'])\n",
    "test_probability_alive.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution for the pobability of alive \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "test_probability_alive['Churn'].plot(kind='hist', bins=50)\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Probability of Churn')\n",
    "plt.title('Customer Probability of Churn Distributaion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "data = test_probability_alive['Churn']\n",
    "\n",
    "N, bins, patches = plt.hist(data, 30, ec=\"k\")\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "low = cmap(0.5)\n",
    "medium =cmap(0.25)\n",
    "high = cmap(0.8)\n",
    "very_high = cmap(1.0)\n",
    "\n",
    "for i in range(0,1):\n",
    "    patches[i].set_facecolor(low)\n",
    "for i in range(1,6):\n",
    "    patches[i].set_facecolor(medium)\n",
    "for i in range(6,15):\n",
    "    patches[i].set_facecolor(high)\n",
    "for i in range(15,30):\n",
    "    patches[i].set_facecolor(very_high)\n",
    "\n",
    "#create legend\n",
    "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [low,medium, high,very_high]]\n",
    "labels= [\"safe\",\"Waving\", \"Accelerated churn\", \"Churned\"]\n",
    "plt.legend(handles, labels)\n",
    "\n",
    "plt.xlabel(\"Probability of Churn\", fontsize=10)  \n",
    "plt.ylabel(\"Number of Customers\", fontsize=10)\n",
    "plt.title('Customer Probability of Churn Distributaion', fontsize=16)\n",
    "plt.xticks(fontsize=14)  \n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.gca().spines[\"top\"].set_visible(False)  \n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portion of waving customers\n",
    "test_probability_alive[(test_probability_alive['Churn'] > 0.05) & ((test_probability_alive['Churn'] <=0.2))].shape[0]/test_probability_alive.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Probability of Churned by Loyalty Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer = pd.read_csv('CHURN_RFM_CUSTOMER.csv')\n",
    "bb = test_probability_alive.reset_index('customer_id')[['customer_id','Churn']]\n",
    "RFM_Customer_Alive = pd.merge(bb, RFM_Customer, how='inner', left_on = 'customer_id', right_on = 'BUSINESS_PARTNER_ID')\n",
    "RFM_Customer_Alive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer_Alive_VIP = RFM_Customer_Alive[RFM_Customer_Alive['LOYALTY_GROUP_MD'] == 'VIP']\n",
    "safe = RFM_Customer_Alive_VIP[(RFM_Customer_Alive_VIP['Churn'] >=0) & (RFM_Customer_Alive_VIP['Churn'] <= 0.05)].count()/RFM_Customer_Alive_VIP.count()\n",
    "Waving = RFM_Customer_Alive_VIP[(RFM_Customer_Alive_VIP['Churn'] >0.05) & (RFM_Customer_Alive_VIP['Churn'] <= 0.2)].count()/RFM_Customer_Alive_VIP.count()\n",
    "Accelerated_Churn = RFM_Customer_Alive_VIP[(RFM_Customer_Alive_VIP['Churn'] >0.2) & (RFM_Customer_Alive_VIP['Churn'] <= 0.5)].count()/RFM_Customer_Alive_VIP.count()\n",
    "Churned = RFM_Customer_Alive_VIP[(RFM_Customer_Alive_VIP['Churn'] >0.5) & (RFM_Customer_Alive_VIP['Churn'] <= 1)].count()/RFM_Customer_Alive_VIP.count()\n",
    "dataframe = {'Safe':[safe.customer_id],'Waving':[Waving.customer_id], 'Accelerated_Churn': [Accelerated_Churn.customer_id], 'Churned':[Churned.customer_id]}\n",
    "\n",
    "vip_dataframe = pd.DataFrame(dataframe)\n",
    "vip_dataframe.rename(index={0: 'VIP'}, inplace= True)\n",
    "vip_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loyal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer_Alive_loyal = RFM_Customer_Alive[RFM_Customer_Alive['LOYALTY_GROUP_MD'] == 'Loyal']\n",
    "safe = RFM_Customer_Alive_loyal[(RFM_Customer_Alive_loyal['Churn'] >=0) & (RFM_Customer_Alive_loyal['Churn'] <= 0.05)].count()/RFM_Customer_Alive_loyal.count()\n",
    "Waving = RFM_Customer_Alive_loyal[(RFM_Customer_Alive_loyal['Churn'] >0.05) & (RFM_Customer_Alive_loyal['Churn'] <= 0.2)].count()/RFM_Customer_Alive_loyal.count()\n",
    "Accelerated_Churn = RFM_Customer_Alive_loyal[(RFM_Customer_Alive_loyal['Churn'] >0.2) & (RFM_Customer_Alive_loyal['Churn'] <= 0.5)].count()/RFM_Customer_Alive_loyal.count()\n",
    "Churned = RFM_Customer_Alive_loyal[(RFM_Customer_Alive_loyal['Churn'] >0.5) & (RFM_Customer_Alive_loyal['Churn'] <= 1)].count()/RFM_Customer_Alive_loyal.count()\n",
    "dataframe = {'Safe':[safe.customer_id],'Waving':[Waving.customer_id], 'Accelerated_Churn': [Accelerated_Churn.customer_id], 'Churned':[Churned.customer_id]}\n",
    "\n",
    "loyal_dataframe = pd.DataFrame(dataframe)\n",
    "loyal_dataframe.rename(index={0: 'LOYAL'}, inplace= True)\n",
    "loyal_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Habitual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer_Alive_Habitual = RFM_Customer_Alive[RFM_Customer_Alive['LOYALTY_GROUP_MD'] == 'Habitual']\n",
    "safe = RFM_Customer_Alive_Habitual[(RFM_Customer_Alive_Habitual['Churn'] >=0) & (RFM_Customer_Alive_Habitual['Churn'] <= 0.05)].count()/RFM_Customer_Alive_Habitual.count()\n",
    "Waving = RFM_Customer_Alive_Habitual[(RFM_Customer_Alive_Habitual['Churn'] >0.05) & (RFM_Customer_Alive_Habitual['Churn'] <= 0.2)].count()/RFM_Customer_Alive_Habitual.count()\n",
    "Accelerated_Churn = RFM_Customer_Alive_Habitual[(RFM_Customer_Alive_Habitual['Churn'] >0.2) & (RFM_Customer_Alive_Habitual['Churn'] <= 0.5)].count()/RFM_Customer_Alive_Habitual.count()\n",
    "Churned = RFM_Customer_Alive_Habitual[(RFM_Customer_Alive_Habitual['Churn'] >0.5) & (RFM_Customer_Alive_Habitual['Churn'] <= 1)].count()/RFM_Customer_Alive_Habitual.count()\n",
    "dataframe = {'Safe':[safe.customer_id],'Waving':[Waving.customer_id], 'Accelerated_Churn': [Accelerated_Churn.customer_id], 'Churned':[Churned.customer_id]}\n",
    "\n",
    "Habitual_dataframe = pd.DataFrame(dataframe)\n",
    "Habitual_dataframe.rename(index={0: 'HABITUAL'}, inplace= True)\n",
    "Habitual_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desirables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer_Alive_Desirables = RFM_Customer_Alive[RFM_Customer_Alive['LOYALTY_GROUP_MD'] == 'Desirables']\n",
    "safe = RFM_Customer_Alive_Desirables[(RFM_Customer_Alive_Desirables['Churn'] >=0) & (RFM_Customer_Alive_Desirables['Churn'] <= 0.05)].count()/RFM_Customer_Alive_Desirables.count()\n",
    "Waving = RFM_Customer_Alive_Desirables[(RFM_Customer_Alive_Desirables['Churn'] >0.05) & (RFM_Customer_Alive_Desirables['Churn'] <= 0.2)].count()/RFM_Customer_Alive_Desirables.count()\n",
    "Accelerated_Churn = RFM_Customer_Alive_Desirables[(RFM_Customer_Alive_Desirables['Churn'] >0.2) & (RFM_Customer_Alive_Desirables['Churn'] <= 0.5)].count()/RFM_Customer_Alive_Desirables.count()\n",
    "Churned = RFM_Customer_Alive_Desirables[(RFM_Customer_Alive_Desirables['Churn'] >0.5) & (RFM_Customer_Alive_Desirables['Churn'] <= 1)].count()/RFM_Customer_Alive_Desirables.count()\n",
    "dataframe = {'Safe':[safe.customer_id],'Waving':[Waving.customer_id], 'Accelerated_Churn': [Accelerated_Churn.customer_id], 'Churned':[Churned.customer_id]}\n",
    "\n",
    "Desirables_dataframe = pd.DataFrame(dataframe)\n",
    "Desirables_dataframe.rename(index={0: 'DESIRABLES'}, inplace= True)\n",
    "Desirables_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_Customer_Alive_Others = RFM_Customer_Alive[RFM_Customer_Alive['LOYALTY_GROUP_MD'].isin(['Irregular','Switchers','Casual / Top-Ups','Unassigned\t'])]\n",
    "safe = RFM_Customer_Alive_Others[(RFM_Customer_Alive_Others['Churn'] >=0) & (RFM_Customer_Alive_Others['Churn'] <= 0.05)].count()/RFM_Customer_Alive_Others.count()\n",
    "Waving = RFM_Customer_Alive_Others[(RFM_Customer_Alive_Others['Churn'] >0.05) & (RFM_Customer_Alive_Others['Churn'] <= 0.2)].count()/RFM_Customer_Alive_Others.count()\n",
    "Accelerated_Churn = RFM_Customer_Alive_Others[(RFM_Customer_Alive_Others['Churn'] >0.2) & (RFM_Customer_Alive_Others['Churn'] <= 0.5)].count()/RFM_Customer_Alive_Others.count()\n",
    "Churned = RFM_Customer_Alive_Others[(RFM_Customer_Alive_Others['Churn'] >0.5) & (RFM_Customer_Alive_Others['Churn'] <= 1)].count()/RFM_Customer_Alive_Others.count()\n",
    "dataframe = {'Safe':[safe.customer_id],'Waving':[Waving.customer_id], 'Accelerated_Churn': [Accelerated_Churn.customer_id], 'Churned':[Churned.customer_id]}\n",
    "\n",
    "Others_dataframe = pd.DataFrame(dataframe)\n",
    "Others_dataframe.rename(index={0: 'OTHERS'}, inplace= True)\n",
    "Others_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = result.plot(kind='bar', stacked=True)\n",
    "plt.ylabel('Portion of Customers by Each Segment')\n",
    "plt.title('Probability of Churned by Loyalty Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Plotting the probability of churn graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes.utils import calculate_alive_path\n",
    "def plot_history_alive(model, t, transactions, datetime_col, freq=\"D\", start_date=None, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Draw a graph showing the probability of being alive for a customer in time.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: lifetimes model\n",
    "        A fitted lifetimes model.\n",
    "    t: int\n",
    "        the number of time units since the birth we want to draw the p_alive\n",
    "    transactions: pandas DataFrame\n",
    "        DataFrame containing the transactions history of the customer_id\n",
    "    datetime_col: str\n",
    "        The column in the transactions that denotes the datetime the purchase was made\n",
    "    freq: str, optional\n",
    "        Default 'D' for days. Other examples= 'W' for weekly\n",
    "    start_date: datetime, optional\n",
    "        Limit xaxis to start date\n",
    "    ax: matplotlib.AxesSubplot, optional\n",
    "        Using user axes\n",
    "    kwargs\n",
    "        Passed into the matplotlib.pyplot.plot command.\n",
    "    Returns\n",
    "    -------\n",
    "    axes: matplotlib.AxesSubplot\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    if start_date is None:\n",
    "        start_date = min(transactions[datetime_col])\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.subplot(111)\n",
    "\n",
    "    # Get purchasing history of user\n",
    "    customer_history = transactions[[datetime_col]].copy()\n",
    "    customer_history.index = pd.DatetimeIndex(customer_history[datetime_col])\n",
    "\n",
    "    # Add transactions column\n",
    "    customer_history[\"transactions\"] = 1\n",
    "    customer_history = customer_history.resample(freq).sum()\n",
    "\n",
    "    # plot alive_path\n",
    "    path = calculate_alive_path(model, transactions, datetime_col, t, freq)\n",
    "    path = 1 - path\n",
    "    path_dates = pd.date_range(start=min(transactions[datetime_col]), periods=len(path), freq=freq)\n",
    "    plt.plot(path_dates, path, \"-\", label=\"Probability of Churn\")\n",
    "\n",
    "    # plot buying dates\n",
    "    payment_dates = customer_history[customer_history[\"transactions\"] >= 1].index\n",
    "    plt.vlines(payment_dates.values, ymin=0, ymax=1, colors=\"r\", linestyles=\"dashed\", label=\"daily purchases\")\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlim(start_date, path_dates[-1])\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel(\"Probability of Churn\")\n",
    "    plt.title(\"History of Probability of Churn\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%Y%m%d\")\n",
    "data_test = data[data['date'] >= '2018-03-01']\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "id = 2100000002\n",
    "days_since_birth = 300\n",
    "sp_trans = data_test.loc[data_test['customer_id'] == id]\n",
    "plot_history_alive(bgf, days_since_birth, sp_trans, 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Customer Probability of Churn Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_alive_path function can calculate the probability of churn for every shopping day for one customer\n",
    "# For example the customer 2101823123, the above graph also shows this custoemr probability of churn changing\n",
    "from lifetimes.utils import calculate_alive_path\n",
    "\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%Y%m%d\")\n",
    "data_test = data[data['date'] >= '2017-10-01']\n",
    "id = 2101275685\n",
    "days_since_birth = 300\n",
    "sp_trans = data_test.loc[data_test['customer_id'] == id]\n",
    "alive_path = pd.DataFrame(1-calculate_alive_path(bgf, sp_trans, 'date', days_since_birth, freq='D'))\n",
    "alive_path.columns = ['Churn']\n",
    "alive_path.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.5 Prediction Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "chunksize = 100000\n",
    "churn = pd.read_csv('data_20171001_20190930.csv', chunksize=100000, iterator=True)\n",
    "data = pd.concat(churn, ignore_index=True)\n",
    "\n",
    "# Manipulate the day range\n",
    "date_range = data[(data['date'] >= 20181001) &(data['date'] <= 20190930)]\n",
    "date_range['date'] = pd.to_datetime(date_range.date, format=\"%Y%m%d\")\n",
    "\n",
    "min=pd.DataFrame(date_range.groupby('customer_id')['date'].min())\n",
    "max=pd.DataFrame(date_range.groupby('customer_id')['date'].max())\n",
    "min_max = pd.concat([min,max], axis=1)\n",
    "min_max['Date_Range'] = min_max.iloc[:,1] - min_max.iloc[:,0]\n",
    "\n",
    "min_max.columns = ['min_date','max_date','date_range']\n",
    "min_max['date_range'] = min_max['date_range'].dt.days\n",
    "min_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predicting the customer average daily spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Delte Outliers for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "# Import the Training and test dataset\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "#Delete the outlier\n",
    "lower_bound = 0.1\n",
    "upper_bound = 0.95\n",
    "training.frequency.quantile([lower_bound,upper_bound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "training['frequency'].plot(kind='hist', bins=50)\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Customer Frequency Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([0,1000])\n",
    "training['monetary_value'].plot(kind='hist', bins=1000)\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Monetary value')\n",
    "plt.title('Customer Monetary value Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking more detail about the monetary value\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([250,1000])\n",
    "plt.ylim([0,2100])\n",
    "training['monetary_value'].plot(kind='hist', bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the outlier\n",
    "training = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)& (training['monetary_value'] <= 400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[['frequency', 'monetary_value']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {'Repeated Daily Purchase':[1.000000,0.014694], 'Average Daily Spend': [1.000000,0.014694]}\n",
    "corr = pd.DataFrame(corr)\n",
    "corr.rename(index = {0:'Repeated Daily Purchase',1:'Average Daily Spend'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit the BG model to the summary_with_money_value dataset\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.08)\n",
    "bgf.fit(training['frequency'], training['recency'], training['T'])\n",
    "bgf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "training = training[training['frequency']>0]\n",
    "training = training[training['monetary_value']>0]\n",
    "\n",
    "from lifetimes import GammaGammaFitter\n",
    "ggf = GammaGammaFitter(penalizer_coef = 0.01)\n",
    "ggf.fit(training['frequency'],\n",
    "        training['monetary_value'])\n",
    "print(ggf.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected conditional average profit: %s, Average profit: %s\" % (\n",
    "    ggf.conditional_expected_average_profit(\n",
    "        training['frequency'],\n",
    "        training['monetary_value']\n",
    "    ).mean(),\n",
    "    training[training['frequency']>0]['monetary_value'].mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predict = ggf.conditional_expected_average_profit(\n",
    "        training['frequency'],\n",
    "        training['monetary_value'])\n",
    "\n",
    "training_actual = training['monetary_value']\n",
    "training_predict_actual = pd.concat([training_actual,training_predict],axis =1)\n",
    "# Rename the column name\n",
    "training_predict_actual.columns = ['Actual', 'Predict']\n",
    "training_predict_actual['difference'] = (training_predict_actual['Predict']-training_predict_actual['Actual'])/training_predict_actual['Actual']*100\n",
    "training_predict_actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All cusotmer \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlim([0,630])\n",
    "plt.ylim([0,630])\n",
    "x = training_predict_actual['Actual'] \n",
    "y = training_predict_actual['Predict']\n",
    "\n",
    "plt.plot(x,y,'r.') # x vs y\n",
    "plt.plot(x,x,'k-') # identity line\n",
    "\n",
    "plt.ylabel('Prediction Monetary Value')\n",
    "plt.xlabel('Actual Monetary Value')\n",
    "plt.title('Expected conditional average spend for Calibration Period')\n",
    "\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_training = training_predict_actual['Predict'].sum()\n",
    "actual_training = training_predict_actual['Actual'].sum()\n",
    "print(\"Total Amount Prediction:\", prediction_training)\n",
    "print(\"Total Amount holdout:\", actual_training)\n",
    "print(\"Difference:\", prediction_training - actual_training)\n",
    "print(\"Prediction Error :\",(prediction_training-actual_training)/actual_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Delte Outliers for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the outlier\n",
    "lower_bound = 0.1\n",
    "upper_bound = 0.95\n",
    "test.frequency.quantile([lower_bound,upper_bound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "%matplotlib inline\n",
    "test['frequency'].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the rfm frequency distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([0,1000])\n",
    "test['monetary_value'].plot(kind='hist', bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detail about monetary value\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([250,1000])\n",
    "plt.ylim([0,2100])\n",
    "test['monetary_value'].plot(kind='hist', bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not delete the outlier and have a try\n",
    "test = test.loc[(test['frequency'] > 0) & (test['frequency'] < 250)& (test['monetary_value'] <= 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.loc[(test['frequency'] > 0) & (test['monetary_value'] > 0)]\n",
    "\n",
    "print(\"Expected conditional average profit: %s, Average profit: %s\" % (\n",
    "    ggf.conditional_expected_average_profit(\n",
    "        test['frequency'],\n",
    "        test['monetary_value']\n",
    "    ).mean(),\n",
    "    test[test['frequency']>0]['monetary_value'].mean()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = ggf.conditional_expected_average_profit(\n",
    "        test['frequency'],\n",
    "        test['monetary_value'])\n",
    "test_actual = test['monetary_value']\n",
    "test_predict_actual = pd.concat([test_actual,test_predict],axis =1)\n",
    "# Rename the column name\n",
    "test_predict_actual.columns = ['Actual', 'Predict']\n",
    "test_predict_actual['difference'] = (test_predict_actual['Predict']-test_predict_actual['Actual'])/test_predict_actual['Actual']\n",
    "test_predict_actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All cusotmer \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xlim([0,620])\n",
    "plt.ylim([0,620])\n",
    "x = test_predict_actual['Actual'] \n",
    "y = test_predict_actual['Predict']\n",
    "\n",
    "plt.plot(x,y,'r.') # x vs y\n",
    "plt.plot(x,x,'k-') # identity line\n",
    "\n",
    "plt.ylabel('Prediction Monetary Value')\n",
    "plt.xlabel('Actual Monetary Value')\n",
    "plt.title('Expected conditional average spend for Holdout Period')\n",
    "\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = test_predict_actual['Predict'].sum()\n",
    "actual_test = test_predict_actual['Actual'].sum()\n",
    "print(\"Total Amount Prediction:\", prediction_test)\n",
    "print(\"Total Amount holdout:\", actual_test)\n",
    "print(\"Difference:\", prediction_test - actual_test)\n",
    "print(\"Prediction Error :\",(prediction_test-actual_test)/actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicting the customer total monetary value in next 1 year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "\n",
    "# refit the BG model to the summary_with_money_value dataset\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "training_model1 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)]\n",
    "bgf.fit(training_model1['frequency'], training_model1['recency'], training_model1['T'])\n",
    "\n",
    "training_model2 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)& (training['monetary_value'] <= 400)]\n",
    "from lifetimes import GammaGammaFitter\n",
    "ggf = GammaGammaFitter(penalizer_coef = 0.05)\n",
    "ggf.fit(training_model2['frequency'],\n",
    "        training_model2['monetary_value'])\n",
    "\n",
    "training = training[(training['frequency'] > 0) & (training['monetary_value'] > 0)]\n",
    "customer_lifetime_value = ggf.customer_lifetime_value(\n",
    "    bgf, #the model to use to predict the number of future transactions\n",
    "    training['frequency'],\n",
    "    training['recency'],\n",
    "    training['T'],\n",
    "    training['monetary_value'],\n",
    "    time=12, # months\n",
    "    discount_rate=0, # Yearly discount rate ~6.0%  p.a  = 0.5% per month  \n",
    "    freq = 'D'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20181001_20190930 = data[(data['date'] >=20181001) & (data['date'] <=20190930)]\n",
    "\n",
    "Monetary_Value_Predict_Actual = pd.concat([customer_lifetime_value, data_20181001_20190930.groupby(['customer_id']).sum()['amount']], axis = 1)\n",
    "Monetary_Value_Predict_Actual.columns = ['Predict', 'Actual']\n",
    "Monetary_Value_Predict_Actual = Monetary_Value_Predict_Actual.dropna()\n",
    "prediction = Monetary_Value_Predict_Actual['Predict'].sum()\n",
    "actual = Monetary_Value_Predict_Actual['Actual'].sum()\n",
    "print(\"Total Amount Prediction:\", prediction)\n",
    "print(\"Total Amount holdout:\", actual)\n",
    "print(\"Difference:\", prediction - actual)\n",
    "print(\"Prediction Error :\",(prediction-actual)/actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All cusotmer \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "ax = Monetary_Value_Predict_Actual.plot(kind='scatter', x='Actual', y='Predict')\n",
    "\n",
    "formatter = ticker.FormatStrFormatter('$%0.1f') #declaring the formatter with the $ sign and y_values with 1 decimalplace\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "x = Monetary_Value_Predict_Actual['Actual'] \n",
    "y = Monetary_Value_Predict_Actual['Predict']\n",
    "plt.plot(x,y,'r.') # x vs y\n",
    "plt.plot(x,x,'k-') # identity line\n",
    "\n",
    "plt.xlim([0,100000])\n",
    "plt.ylim([0,100000])\n",
    "\n",
    "plt.ylabel('Prediction Monetary Value')\n",
    "plt.xlabel('Actual Monetary Value')\n",
    "plt.title('Actual Montary Value VS Prediction Monetary Value')\n",
    "\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference for every customer\n",
    "Monetary_Value_Predict_Actual['Difference'] = round((Monetary_Value_Predict_Actual['Predict'] - Monetary_Value_Predict_Actual['Actual'])/Monetary_Value_Predict_Actual['Actual'],0)\n",
    "difference = pd.DataFrame(Monetary_Value_Predict_Actual[\"Difference\"].value_counts())\n",
    "difference = difference.sort_index(ascending=True)\n",
    "\n",
    "difference_ten = difference.loc[-1:10]\n",
    "difference_ten = difference_ten.append({'Difference' : 28832 } , ignore_index=True)\n",
    "difference_ten.rename(index={0: -1, 1:0, 2: 1, 3:2, 4: 3, 5:4, 6: 5, 7:6, 8: 7, 9:8, 10: 9, 11: 10,12: '10+'}, inplace= True)\n",
    "\n",
    "ax = difference_ten.plot(kind='bar')\n",
    "plt.xlim([-1,13])\n",
    "plt.ylabel('Number of customers')\n",
    "plt.xlabel('Prediction Range')\n",
    "plt.title('Prediction Range Distribution')\n",
    "ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the difference for every customer\n",
    "Monetary_Value_Predict_Actual['Difference'] = round((Monetary_Value_Predict_Actual['Predict'] - Monetary_Value_Predict_Actual['Actual'])/Monetary_Value_Predict_Actual['Actual'],1)\n",
    "difference = pd.DataFrame(Monetary_Value_Predict_Actual[\"Difference\"].value_counts())\n",
    "difference = difference.sort_index(ascending=True)\n",
    "difference_range = difference[(difference['Difference']>=-1) & (difference['Difference']<=1)]\n",
    "\n",
    "ax = difference.plot(kind='line')\n",
    "plt.xlim([-1,1])\n",
    "plt.ylabel('Number of customers')\n",
    "plt.xlabel('Prediction Range')\n",
    "plt.title('Prediction Range Distribution from -1 to 1')\n",
    "ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visullization for next 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "\n",
    "# refit the BG model to the summary_with_money_value dataset\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "training_model1 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)]\n",
    "bgf.fit(training_model1['frequency'], training_model1['recency'], training_model1['T'])\n",
    "\n",
    "training_model2 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)& (training['monetary_value'] <= 400)]\n",
    "from lifetimes import GammaGammaFitter\n",
    "ggf = GammaGammaFitter(penalizer_coef = 0.05)\n",
    "ggf.fit(training_model2['frequency'],\n",
    "        training_model2['monetary_value'])\n",
    "\n",
    "training = training[(training['frequency'] > 0) & (training['monetary_value'] > 0)]\n",
    "customer_lifetime_value_2019 = ggf.customer_lifetime_value(\n",
    "    bgf, #the model to use to predict the number of future transactions\n",
    "    training['frequency'],\n",
    "    training['recency'],\n",
    "    training['T'],\n",
    "    training['monetary_value'],\n",
    "    time=12, # months\n",
    "    discount_rate=0.005, # Yearly discount rate ~6.0%  p.a  = 0.5% per month  \n",
    "    freq = 'D'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "training = pd.read_csv(\"training_day.csv\",index_col='customer_id')\n",
    "test = pd.read_csv(\"test_day.csv\",index_col='customer_id')\n",
    "\n",
    "\n",
    "# refit the BG model to the summary_with_money_value dataset\n",
    "from lifetimes import BetaGeoFitter\n",
    "bgf = BetaGeoFitter(penalizer_coef=0.05)\n",
    "training_model1 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)]\n",
    "bgf.fit(training_model1['frequency'], training_model1['recency'], training_model1['T'])\n",
    "\n",
    "training_model2 = training.loc[(training['frequency'] > 0) & (training['frequency'] < 250)& (training['monetary_value'] <= 400)]\n",
    "from lifetimes import GammaGammaFitter\n",
    "ggf = GammaGammaFitter(penalizer_coef = 0.05)\n",
    "ggf.fit(training_model2['frequency'],\n",
    "        training_model2['monetary_value'])\n",
    "\n",
    "training = training[(training['frequency'] > 0) & (training['monetary_value'] > 0)]\n",
    "customer_lifetime_value_2024 = ggf.customer_lifetime_value(\n",
    "    bgf, #the model to use to predict the number of future transactions\n",
    "    training['frequency'],\n",
    "    training['recency'],\n",
    "    training['T'],\n",
    "    training['monetary_value'],\n",
    "    time=72, # months\n",
    "    discount_rate=0.005, # Yearly discount rate ~6.0%  p.a  = 0.5% per month  \n",
    "    freq = 'D'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total clv 2024 minus total clv 2019\n",
    "customer_lifetime_value_5years = customer_lifetime_value_2024-customer_lifetime_value_2019\n",
    "customer_lifetime_value_5years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlim([0,100000])\n",
    "plt.ylim([0,400000])\n",
    "\n",
    "ax = customer_lifetime_value_5years.plot(kind='hist', bins=1000)\n",
    "\n",
    "formatter = ticker.FormatStrFormatter('$%0.1f') #declaring the formatter with the $ sign and y_values with 1 decimalplace\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Total $dollar Spend from 20191001 to 20240930')\n",
    "plt.title('Customers total $dollar Spend for next 5 years distrbution (6% Discount Rate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the probability of churn and CLV next 5 years\n",
    "clv_5years = pd.concat([customer_lifetime_value_5years,test_probability_alive['Churn']], axis = 1).dropna()\n",
    "\n",
    "x = clv_5years['Churn']\n",
    "y = clv_5years['clv']\n",
    "ax = plt.scatter(x, y, alpha=0.5)\n",
    "\n",
    "plt.ylim([0,750000])\n",
    "plt.ylabel('Total $Spend from 2020 to 2024')\n",
    "plt.xlabel('Probability of Churn')\n",
    "plt.title('Customer next  5 years lifetime value with probability of churn')\n",
    "#ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Waving customers\n",
    "clv_5years_waving = clv_5years[(clv_5years['Churn'] >0.05) & (clv_5years['Churn'] <0.2)]\n",
    "\n",
    "colors = np.where(clv_5years_waving[\"clv\"]>=50000,'r','b')\n",
    "ax = clv_5years_waving.plot(kind='scatter', x='Churn', y='clv',c=colors)\n",
    "\n",
    "formatter = ticker.FormatStrFormatter('$%0.1f') #declaring the formatter with the $ sign and y_values with 1 decimalplace\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "#plt.ylim([0,750000])\n",
    "plt.ylabel('Total $Spend from 2020 to 2024')\n",
    "plt.xlabel('Probability of Waving customer')\n",
    "plt.title('Waving customer next  5 years lifetime value with probability of churn')\n",
    "#ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_5years_waving[clv_5years_waving[\"clv\"]>=50000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Discount \n",
    "a= {'Actual':[4341439234,4287138878, None, None, None, None,None],'Prediction':[None,4289766150,4247675722,4221221544,4201801372,4186447729,4173752048] }\n",
    "a_dataframe = pd.DataFrame(a)\n",
    "a_dataframe\n",
    "a_dataframe.rename(index={0: 2018, 1:2019, 2: 'Next1Year', 3:'Next2Year', 4: 'Next3Year', 5:'Next4Year', 6: 'Next5Year'}, inplace= True)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = a_dataframe.plot(kind='line',marker='o')\n",
    "\n",
    "plt.ylabel('Totale sale (Unit: Billion)')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Customer Lifetime Value for Next 5 Years')\n",
    "#ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Discount = 0.005%\n",
    "a= {'Actual':[4341439234,4287138878, None, None, None, None,None],'Prediction':[None,4153813107,3873999987,3626176354,3399781349,3190559058,2996083484] }\n",
    "a_dataframe = pd.DataFrame(a)\n",
    "a_dataframe\n",
    "a_dataframe.rename(index={0: 2018, 1:2019, 2: 'Next1Year', 3:'Next2Year', 4: 'Next3Year', 5:'Next4Year', 6: 'Next5Year'}, inplace= True)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = a_dataframe.plot(kind='line',marker='o')\n",
    "\n",
    "plt.ylabel('Totale sale (Unit: Billion)')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Customer Lifetime Value for Next 5 Years with 6% Discount Rate')\n",
    "#ax.legend().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
